# examples/hybrid/two_tower_hybrid_example.py
import pandas as pd
import numpy as np
import os
import sys
import tensorflow as tf
from tensorflow.keras.layers import Input, Embedding, Flatten, Dense, Concatenate, GlobalAveragePooling1D, Dot
from tensorflow.keras.models import Model
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.preprocessing.text import Tokenizer
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split

# --- Two-Tower Hybrid Recommendation Model: Basic Explanation ---
# The Two-Tower model is a popular architecture for recommendation systems, especially for candidate generation
# in large-scale systems. It learns separate representations (embeddings) for users and items in two "towers"
# and then combines them to predict user-item interaction.
#
# How it works:
# 1. User Tower:
#    - Input: User features (e.g., user ID, demographics, user activity history embeddings).
#    - Architecture: Typically consists of embedding layers for categorical features, dense layers for numerical
#      features, and then a few hidden dense layers to produce a final user embedding vector.
#    - For this example, we'll keep it simple: User ID -> User Embedding.
# 2. Item Tower:
#    - Input: Item features (e.g., item ID, item category, textual descriptions, image embeddings).
#    - Architecture: Similar to the user tower, it uses embedding layers for item ID and other categorical
#      features, processes text/image features (e.g., using CNNs, RNNs, or pre-trained embeddings),
#      and then a few hidden dense layers to produce a final item embedding vector.
#    - For this example: Item ID -> Item Embedding, Item Genres -> Genre Embeddings (pooled).
# 3. Interaction & Prediction:
#    - The user embedding (from the user tower) and item embedding (from the item tower) are combined.
#    - A common method is to compute the dot product (or cosine similarity) between the user and item embeddings.
#      This dot product represents the predicted affinity or score for the user-item pair.
# 4. Training:
#    - The model is typically trained on pairs of (user, item) interactions.
#    - Positive pairs are actual interactions (e.g., clicks, purchases).
#    - Negative pairs are non-interactions, often generated by negative sampling.
#    - Loss Function: Binary cross-entropy is common if predicting a probability (after sigmoid on dot product),
#      or hinge loss for ranking tasks.
# 5. Serving / Recommendation:
#    - User Tower generates user embedding for a given user.
#    - Item Tower generates item embeddings for all candidate items (can be precomputed).
#    - An Approximate Nearest Neighbor (ANN) search is performed to find items whose embeddings are "closest"
#      (highest dot product) to the user embedding.
#
# Pros:
# - Scalability: User and item embeddings can be computed independently. Item embeddings can be precomputed
#   and indexed in an ANN system for fast retrieval during serving.
# - Flexibility: Easy to incorporate various types of user and item features into respective towers.
# - Good for Candidate Generation: Efficiently narrows down a large item corpus to a smaller set of relevant candidates.
#
# Cons:
# - Interaction Modeling: The final interaction is often just a dot product, which might not capture very complex
#   user-item relationships as effectively as models that allow more intricate feature crossing.
# - Feature Engineering: Performance still relies on good feature engineering within each tower.
# ---

# Dynamically add project root to sys.path
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

# --- Model Hyperparameters ---
USER_EMBEDDING_DIM = 32
ITEM_EMBEDDING_DIM = 32
GENRE_EMBEDDING_DIM = 16
MAX_GENRES_PER_ITEM = 5 # Max number of genres to consider per item for padding
DENSE_UNITS = [64, USER_EMBEDDING_DIM] # Ensure last dense layer of item tower matches user embedding dim
LEARNING_RATE = 0.001
EPOCHS = 5
BATCH_SIZE = 64
NEGATIVE_SAMPLES = 4 # Number of negative samples per positive one

def load_and_preprocess_data(
    base_interactions_filepath='data/dummy_interactions.csv',
    base_metadata_filepath='data/dummy_item_metadata.csv'
):
    """Loads interactions and metadata, preprocesses for two-tower model."""
    interactions_filepath = os.path.join(project_root, base_interactions_filepath)
    metadata_filepath = os.path.join(project_root, base_metadata_filepath)

    # Ensure data exists, generate if not
    for fp_abs, fp_rel in [(interactions_filepath, base_interactions_filepath),
                           (metadata_filepath, base_metadata_filepath)]:
        if not os.path.exists(fp_abs):
            print(f"오류: {fp_abs} ({fp_rel}) 파일을 찾을 수 없습니다.")
            try:
                from data.generate_dummy_data import generate_dummy_data
                # Two-tower doesn't need sequences by default, ensure other files are robustly generated
                print("더미 데이터 생성 시도 중... (generate_sequences=False)")
                generate_dummy_data(num_users=200, num_items=100, num_interactions=2000, generate_sequences=False)
                print("더미 데이터 생성 완료.")
            except Exception as e:
                print(f"더미 데이터 생성 실패: {e}")
                return None # Indicate failure
            if not os.path.exists(fp_abs): # Check again after generation attempt
                 print(f"파일 생성 실패 후에도 {fp_abs} 찾을 수 없음")
                 return None

    df_interactions = pd.read_csv(interactions_filepath)
    df_items_meta = pd.read_csv(metadata_filepath)

    if df_interactions.empty or df_items_meta.empty:
        print("데이터 파일이 비어있거나 로드에 실패했습니다.")
        return None

    # Ensure item_id is string type for consistent encoding and lookup
    df_interactions['item_id'] = df_interactions['item_id'].astype(str)
    df_items_meta['item_id'] = df_items_meta['item_id'].astype(str)

    # Encode User IDs
    user_encoder = LabelEncoder()
    df_interactions['user_idx'] = user_encoder.fit_transform(df_interactions['user_id'])
    num_users = len(user_encoder.classes_)

    # Encode Item IDs (consistent across interactions and metadata)
    all_item_ids = pd.concat([
        df_interactions['item_id'],
        df_items_meta['item_id']
    ]).astype(str).unique() # Ensure consistent type (str) before encoding
    item_encoder = LabelEncoder()
    item_encoder.fit(all_item_ids)

    df_interactions['item_idx'] = item_encoder.transform(df_interactions['item_id'].astype(str))
    df_items_meta['item_idx'] = item_encoder.transform(df_items_meta['item_id'].astype(str))
    num_items = len(item_encoder.classes_)

    # Preprocess Genres
    df_items_meta['genres_list'] = df_items_meta['genres'].fillna('').astype(str).apply(lambda x: x.split(';'))

    genre_tokenizer = Tokenizer(oov_token="<unk>") # Handle unknown genres
    genre_tokenizer.fit_on_texts(df_items_meta['genres_list'])

    item_genre_sequences = genre_tokenizer.texts_to_sequences(df_items_meta['genres_list'])
    item_genre_padded = pad_sequences(item_genre_sequences, maxlen=MAX_GENRES_PER_ITEM, padding='post', truncating='post', value=0) # 0 for padding

    num_genres = len(genre_tokenizer.word_index) + 1 # +1 for padding token 0

    item_idx_to_genres = {row['item_idx']: item_genre_padded[i] for i, row in df_items_meta.iterrows()}

    # Create training data
    positive_samples = df_interactions[['user_idx', 'item_idx']].copy()
    positive_samples['label'] = 1.0 # Use float for labels

    negative_samples_list = []
    all_possible_item_indices = np.arange(num_items) # These are 0 to num_items-1

    user_item_interactions = set(zip(positive_samples['user_idx'], positive_samples['item_idx']))

    for _, row in positive_samples.iterrows():
        user_idx = row['user_idx']
        count = 0
        while count < NEGATIVE_SAMPLES:
            negative_item_idx = np.random.choice(all_possible_item_indices)
            if (user_idx, negative_item_idx) not in user_item_interactions:
                negative_samples_list.append({'user_idx': user_idx, 'item_idx': negative_item_idx, 'label': 0.0})
                count += 1

    df_negative_samples = pd.DataFrame(negative_samples_list)
    training_data = pd.concat([positive_samples, df_negative_samples]).sample(frac=1, random_state=42).reset_index(drop=True)

    train_df, test_df = train_test_split(training_data, test_size=0.2, random_state=42, stratify=training_data['label'])

    return (train_df, test_df,
            user_encoder, item_encoder, genre_tokenizer,
            num_users, num_items, num_genres,
            item_idx_to_genres, df_items_meta)


def build_two_tower_model(num_users, num_items, num_genres):
    """Builds the Two-Tower model: user_tower, item_tower, and training_model."""
    # --- User Tower ---
    user_input_idx = Input(shape=(1,), name='user_input_idx', dtype='int32')
    user_embedding_layer = Embedding(num_users, USER_EMBEDDING_DIM, name='user_embedding')
    user_vector = Flatten()(user_embedding_layer(user_input_idx))
    # Optional Dense layers for user tower (could be added here)
    user_model = Model(inputs=user_input_idx, outputs=user_vector, name='user_tower')

    # --- Item Tower ---
    item_id_input_idx = Input(shape=(1,), name='item_id_input_idx', dtype='int32')
    item_genre_input_seq = Input(shape=(MAX_GENRES_PER_ITEM,), name='item_genre_input_seq', dtype='int32')

    item_id_embedding_layer = Embedding(num_items, ITEM_EMBEDDING_DIM, name='item_id_embedding')
    item_id_vector = Flatten()(item_id_embedding_layer(item_id_input_idx))

    genre_embedding_layer = Embedding(num_genres, GENRE_EMBEDDING_DIM, name='genre_embedding', mask_zero=False) # mask_zero=False as 0 is a valid token from Tokenizer
    genre_vectors = genre_embedding_layer(item_genre_input_seq)
    genre_pooled_vector = GlobalAveragePooling1D()(genre_vectors)

    concatenated_item_features = Concatenate()([item_id_vector, genre_pooled_vector])

    current_layer = concatenated_item_features
    for i, units in enumerate(DENSE_UNITS):
        activation = 'relu' if i < len(DENSE_UNITS) -1 else 'linear' # Linear for last layer before dot product
        # The last dense layer of item tower should output USER_EMBEDDING_DIM to match user tower for dot product
        current_layer = Dense(units, activation=activation, name=f'item_dense_{i}')(current_layer)
    item_vector = current_layer # This is now the item embedding

    item_model = Model(inputs=[item_id_input_idx, item_genre_input_seq], outputs=item_vector, name='item_tower')

    # --- Combine Towers for Training ---
    # Training model inputs must match the keys passed in model.fit()
    user_input_train = Input(shape=(1,), name='user_idx_train_input', dtype='int32') # Name matches key
    item_id_input_train = Input(shape=(1,), name='item_idx_train_input', dtype='int32') # Name matches key
    item_genre_input_train = Input(shape=(MAX_GENRES_PER_ITEM,), name='genre_seq_train_input', dtype='int32') # Name matches key

    user_embedding_train = user_model(user_input_train)
    item_embedding_train = item_model([item_id_input_train, item_genre_input_train])

    dot_product = Dot(axes=1, normalize=False)([user_embedding_train, item_embedding_train])
    output = Dense(1, activation='sigmoid', name='interaction_output')(dot_product)

    training_model = Model(
        inputs={'user_idx_train_input': user_input_train,
                'item_idx_train_input': item_id_input_train,
                'genre_seq_train_input': item_genre_input_train},
        outputs=output,
        name='two_tower_training_model'
    )
    return training_model, user_model, item_model


def get_two_tower_recommendations(user_id_original, user_encoder, item_encoder,
                                  user_model, item_model,
                                  item_idx_to_genres, all_item_indices_encoded,
                                  num_recommendations=5):
    """Generates recommendations for a user."""
    try:
        user_idx = user_encoder.transform(np.array([user_id_original]))[0] # Ensure input is array-like
    except ValueError:
        print(f"User ID {user_id_original} not found in encoder.")
        return []

    user_embedding = user_model.predict(np.array([user_idx]), verbose=0)

    all_item_ids_input = all_item_indices_encoded.reshape(-1,1)
    all_item_genres_input = np.array([item_idx_to_genres.get(idx, [0]*MAX_GENRES_PER_ITEM) for idx in all_item_indices_encoded])

    all_item_embeddings = item_model.predict([all_item_ids_input, all_item_genres_input], batch_size=BATCH_SIZE, verbose=0)

    similarities = np.dot(all_item_embeddings, user_embedding.T).flatten()

    top_n_indices_in_all_items_array = np.argsort(similarities)[-num_recommendations:][::-1]

    recommendations = []
    for arr_idx in top_n_indices_in_all_items_array:
        original_item_idx_encoded = all_item_indices_encoded[arr_idx]
        original_item_id = item_encoder.inverse_transform([original_item_idx_encoded])[0]
        recommendations.append({
            'item_id': original_item_id,
            'similarity_score': similarities[arr_idx]
        })
    return recommendations

# --- Main Execution ---
if __name__ == "__main__":
    print("--- Two-Tower Hybrid Recommender 예제 시작 ---")

    print("\n데이터 로드 및 전처리 중...")
    result = load_and_preprocess_data()

    if result:
        (train_df, test_df,
         user_encoder, item_encoder, genre_tokenizer,
         num_users, num_items, num_genres,
         item_idx_to_genres, df_items_meta) = result # Added df_items_meta to the return

        print(f"학습 데이터: {len(train_df)} 샘플, 테스트 데이터: {len(test_df)} 샘플")
        print(f"고유 사용자: {num_users}, 고유 아이템: {num_items}, 고유 장르 토큰: {num_genres}")

        print("\nTwo-Tower 모델 구축 중...")
        training_model, user_model, item_model = build_two_tower_model(num_users, num_items, num_genres)

        training_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),
                               loss='binary_crossentropy',
                               metrics=['accuracy'])
        training_model.summary()

        # Prepare training data inputs for the model as a dictionary
        train_inputs = {
            'user_idx_train_input': train_df['user_idx'].values.astype(np.int32),
            'item_idx_train_input': train_df['item_idx'].values.astype(np.int32),
            'genre_seq_train_input': np.array([item_idx_to_genres.get(idx, [0]*MAX_GENRES_PER_ITEM) for idx in train_df['item_idx']]).astype(np.int32)
        }
        train_labels = train_df['label'].values.astype(np.float32)

        test_inputs = {
            'user_idx_train_input': test_df['user_idx'].values.astype(np.int32),
            'item_idx_train_input': test_df['item_idx'].values.astype(np.int32),
            'genre_seq_train_input': np.array([item_idx_to_genres.get(idx, [0]*MAX_GENRES_PER_ITEM) for idx in test_df['item_idx']]).astype(np.int32)
        }
        test_labels = test_df['label'].values.astype(np.float32)

        print("\n모델 학습 중...")
        history = training_model.fit(
            train_inputs,
            train_labels,
            epochs=EPOCHS,
            batch_size=BATCH_SIZE,
            validation_data=(test_inputs, test_labels),
            verbose=1
        )
        print("모델 학습 완료.")

        if hasattr(user_encoder, 'classes_') and user_encoder.classes_.size > 0:
            target_user_original_id = user_encoder.classes_[0]

            all_item_indices_encoded = item_encoder.transform(item_encoder.classes_)

            print(f"\n사용자 ID {target_user_original_id}에 대한 추천 생성 중...")
            recommendations = get_two_tower_recommendations(
                target_user_original_id, user_encoder, item_encoder,
                user_model, item_model,
                item_idx_to_genres, all_item_indices_encoded,
                num_recommendations=5
            )

            print("\n추천된 아이템:")
            if recommendations:
                for rec in recommendations:
                    # For more details, merge with item metadata
                    item_details = df_items_meta[df_items_meta['item_id'] == rec['item_id']]
                    desc = item_details['description'].iloc[0] if not item_details.empty else "N/A"
                    print(f"- 아이템 {rec['item_id']} (유사도 점수: {rec['similarity_score']:.4f}): {desc[:60]}...")
            else:
                print("추천할 아이템이 없습니다.")
        else:
            print("\n추천을 생성할 사용자가 없습니다.")
    else:
        print("\n데이터 로드 및 전처리에 실패하여 예제를 실행할 수 없습니다.")

    print("\n--- Two-Tower Hybrid Recommender 예제 실행 완료 ---")
